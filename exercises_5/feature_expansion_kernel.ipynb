{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Painting style recognition from images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will evaluate linear regression, without and with feature expansion, kernel regression and kernel SVM on the task of painting style recognition from images. In the SVM case, we will make use of scikit-learn.\n",
    "\n",
    "We will use a subset of the wikiart dataset of Tan et al., A Deep Convolutional Network for Fine-art Paintings Classification, ICIP 2016. This subset consists of 64x64 images of paintings of 8 different styles (Abstract-Expressionism, Art-Nouveau Modern, Baroque, Color Field Painting, Cubism, Early Renaissance, Expressionism, High Renaissance). There are between 1343 and 2782 examples per class. I precomputed features (Histogram of Oriented Gradient) from the images. The data is given in two parts (because of Moodle size limitations). Each part contains:\n",
    "- X: the feature vector for each image\n",
    "- Y: the label of each image\n",
    "\n",
    "Let us first load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.io as sio # This will allow us to load the data\n",
    "data = sio.loadmat('wikiart_data/wikiart_data_1.mat')\n",
    "X1 = data['X1']\n",
    "Y1 = data['Y1']\n",
    "data = sio.loadmat('wikiart_data/wikiart_data_2.mat')\n",
    "X2 = data['X2']\n",
    "Y2 = data['Y2']\n",
    "X = np.vstack((X1,X2))\n",
    "Y = np.vstack((Y1,Y2))\n",
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us visualize the first image of each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(2, 4)\n",
    "os.chdir('wikiart_samples')\n",
    "for i in range(0,2):\n",
    "    for j in range(0,4):\n",
    "        dirnum = i*4+j+1\n",
    "        os.chdir(str(dirnum))\n",
    "        name = os.listdir()\n",
    "        img = plt.imread(name[0])\n",
    "        ax[i,j].imshow(img)\n",
    "        os.chdir('..')\n",
    "os.chdir('..')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then separate the data between training and test samples. To this end, we will make use of the scikit-learn train_test_split function, and keep one third of the data as test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "Xtrain,Xtest,Ytrain,Ytest = train_test_split(X,Y,test_size=0.33,random_state=1)\n",
    "Ytrain = Ytrain.squeeze()\n",
    "Ytest = Ytest.squeeze()\n",
    "print(Xtrain.shape)\n",
    "print(Xtest.shape)\n",
    "print(Ytrain.shape)\n",
    "print(Ytest.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us compute one-hot encodings of the training labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ytrain_oh = np.zeros((Ytrain.shape[0],8))\n",
    "Ytrain_oh[(np.arange(Ytrain_oh.shape[0]),Ytrain.flatten()-1)] = 1\n",
    "print(Ytrain_oh.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us first look at linear regression for a linear baseline. The first thing to do is to add a 1 to the inputs to account for the bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xbtrain = np.hstack((np.ones((Xtrain.shape[0],1)),Xtrain))\n",
    "Xbtest = np.hstack((np.ones((Xtest.shape[0],1)),Xtest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Then, we can compute the optimal parameter matrix W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = np.linalg.pinv(Xbtrain)\n",
    "W = M@Ytrain_oh\n",
    "print(W.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From these weights, we can compute the predicted class score vectors, and convert them to labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Yhat_oh = Xbtest@W\n",
    "Yhat = np.argmax(Yhat_oh,axis=1)+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can compute the confusion matrix and the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics as skm\n",
    "cmat = skm.confusion_matrix(Ytest,Yhat)\n",
    "cmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.diag(cmat).sum()/cmat.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature expansion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now look at expanding the features. To this end, we will consider quadratic expansion, but, because of the size of the input, limit ourselves to the squares of individual variables (i.e., we will not consider products of the form $x_i^{(j)}x_i^{(k)}$). Note that we will still use the original features in addition to the quadratic ones. Let us first compute the expanded features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Expand the original features with quadratic ones\n",
    "Phibtrain = np.hstack((Phitrain,np.ones((Phitrain.shape[0],1))))\n",
    "Phibtest = np.hstack((Phitest,np.ones((Phitest.shape[0],1))))\n",
    "print(Phibtrain.shape)\n",
    "print(Phibtest.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then again apply linear regression to the resulting features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = np.linalg.pinv(Phibtrain)\n",
    "W = M@Ytrain_oh\n",
    "Yhat_oh = Phibtest@W\n",
    "Yhat = np.argmax(Yhat_oh,axis=1)+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can compute the confusion matrix and the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmat = skm.confusion_matrix(Ytest,Yhat)\n",
    "cmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.diag(cmat).sum()/cmat.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is disappointing: It means that, with the additional features, the model is able to better fit to the training data, without generalizing to the test data (you can see this by evaluating the trained models on the training data instead of the testing one). This is a case of \"overfitting\", which we will discuss next week. Feel free to experiment with other expansion strategies, such as higher polynomial degrees and sine/cosine functions, to see if you can improve this result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now look at kernel regression for classification. We will try with both a quadratic polynomial kernel and an RBF one. Let us start with the polynomial one. First, we need to compute the kernel matrices (training and test). To this end, make use of the data augmented with the additional 1. Note that the quadratic kernel is not equivalent to our previous quadratic feature expansion, because it implicitly encompasses all pairwise products between the variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compute the training and testing kernel matrices for a quadratic kernel\n",
    "print(K.shape)\n",
    "print(Kt.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then compute the scores for the test data and convert them into labels. Note that, in practice, the kernel matrix is quite large, and is therefore likely to have low rank and thus not be invertible. To overcome this, we can add a small value, e.g., 1e-3, on its diagonal. This may seem like a heuristic but is in fact justified, as we will discuss next week during the lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compute the predictions (Yhat_oh) using the closed-form solution of kernel regression\n",
    "Yhat = np.argmax(Yhat_oh,axis=1)+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the confusion matrix and the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmat = skm.confusion_matrix(Ytest,Yhat)\n",
    "cmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.diag(cmat).sum()/cmat.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now try with an RBF kernel. Play with the value $\\sigma^2$ used in this kernel. First, compute the training and test kernel matrices. Note that the pairwise distances between the samples can be computed using the scikit-learn function sklearn.metrics.pairwise.pairwise_distances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compute the training and testing kernel matrices for an RBF kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the scores for the test data and convert them to labels. Again, when inverting the kernel matrix, add a small value to its diagonal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compute the predictions (Yhat_oh) using the closed-form solution of kernel regression\n",
    "Yhat = np.argmax(Yhat_oh,axis=1)+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the confusion matrix and the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmat = skm.confusion_matrix(Ytest,Yhat)\n",
    "cmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.diag(cmat).sum()/cmat.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now look at linear and kernel SVM (in scikit-learn). Recall that, with scikit-learn, you don't need to add the 1 to the input features because this is handled within the SVM implementation. In both cases, evaluate the influence of the hyper-parameter $C$ that balances the margin-related term with the one penalizing large slack variables. Start with the linear case, using LinearSVC, by fitting the classifier to the training data (N.B. Do not worry too much about the warning regarding the number of iterations; training is already long and it seems that we are in fact close to convergence)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "clf = svm.LinearSVC(loss='hinge',C=1)\n",
    "clf.fit(Xtrain, Ytrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can then use the classifier to predict the labels for the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Yhat = clf.predict(Xtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, compute the confusion matrix and the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmat = skm.confusion_matrix(Ytest,Yhat)\n",
    "cmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.diag(cmat).sum()/cmat.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With linear SVM, we can still use feature expansion. Let's re-use the same quadratically-expanded features as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Train a linear SVM with the same expanded features as before"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can then use the classifier to predict the labels for the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Yhat = clf.predict(Phitest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, compute the confusion matrix and the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmat = skm.confusion_matrix(Ytest,Yhat)\n",
    "cmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.diag(cmat).sum()/cmat.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At least, this time, we can get slightly better results than with the original features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now let us apply kernel SVM with an RBF kernel. To this end, you need to use the SVC function of scikit-learn. Evaluate the influence of $\\sigma$ (gamma in scikit-learn, hint: around 0.1 seems reasonable). Again, first fit the classifier to the training data, and then use it to predict the labels for the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Train a kernel SVM with an RBF kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Yhat = clf.predict(Xtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, compute the confusion matrix and the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmat = skm.confusion_matrix(Ytest,Yhat)\n",
    "cmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.diag(cmat).sum()/cmat.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
