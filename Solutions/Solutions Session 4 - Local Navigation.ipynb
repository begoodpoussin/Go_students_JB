{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div style=\"border:1px solid black; padding:20px 20px;text-align: justify;text-justify: inter-word\">\n",
    "    <strong>Solutions Session 4 - Local Navigation<br/> Duration : 4 hours (2 in session + 2 at home)</strong><br/><br/>\n",
    "    <span style=\"text-decoration:underline;font-weight:bold;\">How to use this notebook?</span><br/>\n",
    "    This notebook is made of text cells and code cells. The code cells have to be <strong>executed</strong> to see the result of the program. To execute a cell, simply select it and click on the \"play\" button (<span style=\"font: bold 12px/30px Arial, serif;\">&#9658;</span>) in the tool bar just above the notebook, or type <code>shift + enter</code>. It is important to execute the code cells in their order of appearance in the notebook.<br/>\n",
    "You can make use of the table of contents to navigate easily between sections.\n",
    "</div>\n",
    "\n",
    "<br/>\n",
    "\n",
    "<div style=\"justify;text-justify: inter-word\">\n",
    "So that you may familiarise with the notebooks and the basic python syntax, the exercises are provided in notebook form and whenever there are any calculations to be made, we encourage you to do them by code. Also, if you want to take notes, we encourage you to use the markdown or Raw NBConvert cells. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Learning-Goals\" data-toc-modified-id=\"Learning-Goals-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Learning Goals</a></span></li><li><span><a href=\"#Requirements\" data-toc-modified-id=\"Requirements-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Requirements</a></span></li><li><span><a href=\"#Proximity-Sensors\" data-toc-modified-id=\"Proximity-Sensors-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Proximity Sensors</a></span><ul class=\"toc-item\"><li><span><a href=\"#Characterising-the-Proximity-Sensors\" data-toc-modified-id=\"Characterising-the-Proximity-Sensors-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Characterising the Proximity Sensors</a></span></li></ul></li><li><span><a href=\"#Local-Navigation-on-Thymio\" data-toc-modified-id=\"Local-Navigation-on-Thymio-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Local Navigation on Thymio</a></span><ul class=\"toc-item\"><li><span><a href=\"#Moving-Toward-the-Goal\" data-toc-modified-id=\"Moving-Toward-the-Goal-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Moving Toward the Goal</a></span></li><li><span><a href=\"#Local-Obstacle-Avoidance\" data-toc-modified-id=\"Local-Obstacle-Avoidance-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Local Obstacle Avoidance</a></span></li><li><span><a href=\"#Potential-Field-Navigation\" data-toc-modified-id=\"Potential-Field-Navigation-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>Potential Field Navigation</a></span></li></ul></li><li><span><a href=\"#(Optional)-Using-Sensor-Values-and-Displacements-to-Map-the-Environment\" data-toc-modified-id=\"(Optional)-Using-Sensor-Values-and-Displacements-to-Map-the-Environment-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>(Optional) Using Sensor Values and Displacements to Map the Environment</a></span><ul class=\"toc-item\"><li><span><a href=\"#Creating-a-local-occupancy-grid-based-on-the-individual-sensor-values\" data-toc-modified-id=\"Creating-a-local-occupancy-grid-based-on-the-individual-sensor-values-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Creating a local occupancy grid based on the individual sensor values</a></span></li><li><span><a href=\"#Creating-the-global-map-provided-the-sensor-values-and-robot-displacements-within-the-map.\" data-toc-modified-id=\"Creating-the-global-map-provided-the-sensor-values-and-robot-displacements-within-the-map.-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>Creating the global map provided the sensor values and robot displacements within the map.</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Learning Goals\n",
    "\n",
    "\n",
    "- Proximity sensor analysis and local navigation implementation on Thymio\n",
    "\n",
    "\n",
    "- Analysing the proximity sensors\n",
    "\n",
    "\n",
    "- Creating a local occupancy grid and global map from sensor values and displacements (Jupyter notebooks)\n",
    "\n",
    "\n",
    "- Implementing local navigation on the Thymio  (ASEBA studio) in real time\n",
    "\n",
    "\n",
    "# Requirements\n",
    "\n",
    "- Thymio \n",
    "\n",
    "- The gradient printed on an A3 paper\n",
    "\n",
    "\n",
    "![gradient](images/gradient.png)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-28T12:23:28.161708Z",
     "start_time": "2022-10-28T12:23:22.702575Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tdmclient in c:\\users\\begoo\\miniconda3\\lib\\site-packages (0.1.19)\n",
      "Requirement already satisfied: zeroconf in c:\\users\\begoo\\miniconda3\\lib\\site-packages (from tdmclient) (0.39.1)\n",
      "Requirement already satisfied: websockets in c:\\users\\begoo\\miniconda3\\lib\\site-packages (from tdmclient) (10.3)\n",
      "Requirement already satisfied: ifaddr>=0.1.7 in c:\\users\\begoo\\miniconda3\\lib\\site-packages (from zeroconf->tdmclient) (0.2.0)\n",
      "Requirement already satisfied: async-timeout>=4.0.1 in c:\\users\\begoo\\miniconda3\\lib\\site-packages (from zeroconf->tdmclient) (4.0.2)\n",
      "Requirement already satisfied: typing-extensions>=3.6.5 in c:\\users\\begoo\\miniconda3\\lib\\site-packages (from async-timeout>=4.0.1->zeroconf->tdmclient) (4.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade tdmclient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proximity Sensors\n",
    "\n",
    "## Characterising the Proximity Sensors\n",
    "\n",
    "Proximity sensors just measure the presence of objects, and are hard to characterise in that very fuzzy function. Consider here the use of proximity sensors as distance sensors for a very well defined situation (facing a white paper). \n",
    "\n",
    "\n",
    "<blockquote>\n",
    "<span style=\"color: #2980B9 ;\">\n",
    "    \n",
    "On the Thymio schematics one can find that the sensor is a ITR9909.   Have a look [here](http://www.mgelectronic.rs/ProductFilesDownload?Id=2470) for the datasheet. \n",
    "\n",
    "Thymio has seven horizontal proximity sensors of which five are located in the front and two in the rear as can be seen in the image below.\n",
    "\n",
    "![Sensors](images/sensors.svg)\n",
    "\n",
    "For this exericse, we used the proximity sensors in the front and white obstacle to measure the distances.\n",
    "\n",
    "</span>\n",
    "</blockquote>\n",
    "\n",
    "\n",
    "***In this context, is it possible to extract the position from the data acquired by the proximity sensor?***\n",
    "\n",
    "\n",
    "<blockquote>\n",
    "<span style=\"color: #2980B9 ;\">\n",
    "    \n",
    "Yes, it is possible to extract the position from the proximity sensors. \n",
    "\n",
    "\n",
    "***Why?***  We can do this since the proximity sensors measure the intensity of the infrared light reflected by an obstacle in front of it. An obstacle close to the sensor will reflect more light compared to an obstacle far away from the sensor. With this one can infer an approximate distance relative to the thymio using proximity sensors.\n",
    "\n",
    "</span>\n",
    "</blockquote>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***If yes, define an approximate value of the range, the dynamic range, the update frequency, the precision, the resolution, and the accuracy of this specific distance sensor.***\n",
    "\n",
    "<blockquote>\n",
    "<span style=\"color: #2980B9 ;\">\n",
    "    \n",
    "\n",
    "In order to measure the sensor values we first load the following code into thymio. The code reads and plots all the horizontal proximity sensors in Aseba Studio software. By placing a white obstacle in front of the sensor one can observe the proximity sensor values in the plot. For more accurate values the \\textit{variables} dialog in the Aseba Studio can be used.\n",
    "\n",
    "\n",
    "    var run=0\n",
    "\n",
    "    onevent button.forward\n",
    "        run=1\n",
    "\n",
    "    onevent button.center\n",
    "        run=0\n",
    "\n",
    "    onevent prox\n",
    "        if  run==1 then\n",
    "            emit plot_event prox.horizontal\n",
    "        end\n",
    "\n",
    "\n",
    "\n",
    "We can construct a table to map sensor values to distance. The table below is based upon real measurements. Measurements were made every 1 cm up until 14 cm. After 14 cm, the sensors no longer detects the objects (value = 0). The second column of the table shows the value of the sensor for each distance. The collected data is used to linearize and obtain the co-efficients of the linearized line to convert sensor values to absolute distance. The linearized equation is,\n",
    "\t\n",
    "\\begin{equation}\n",
    "  {\\rm distance} = -339.47*(sensor\\_value) + 5739.9  \n",
    "\\end{equation}\n",
    "\t\n",
    "\t\n",
    "    \n",
    "| True Distance [cm] | Sensor Value | Computed Distance [cm] |\n",
    "|:---------------------------:|:---------------------:|:-------------------------------:|\n",
    "|              1              |          4996         |           2.191357116           |\n",
    "|              2              |          5007         |           2.158953663           |\n",
    "|              3              |          4935         |           2.371048988           |\n",
    "|              4              |          4554         |            3.49338675           |\n",
    "|              5              |          4018         |           5.072318614           |\n",
    "|              6              |          3624         |           6.232951365           |\n",
    "|              7              |          3292         |           7.210946475           |\n",
    "|              8              |          2987         |           8.109405839           |\n",
    "|              9              |          2800         |            8.66026453           |\n",
    "|              10             |          2580         |           9.308333579           |\n",
    "|              11             |          2307         |           10.11252835           |\n",
    "|              12             |          2039         |           10.90199429           |\n",
    "|              13             |          1575         |           12.26883082           |\n",
    "|              14             |           0           |                0                |\n",
    "        \n",
    "\n",
    "- ***Range*** : \n",
    "    \n",
    "    The range of the sensor is approximately between [3cm,13cm]. Below 3cm one can observe that the value remains constant which indicates a saturation of the sensor measurement. On the other hand above 13cm one can observe that there is no detection of the obstacle anymore i.e. the sensor now reads a zero.\n",
    "\n",
    "\n",
    "- ***Dynamic Range***\n",
    "    \n",
    "    The Dynamic Range is defined as, \n",
    "    \n",
    "    \\begin{equation}\n",
    "        D = 20 \\log_{10}(\\frac{x_{FS}}{\\Delta x})\n",
    "    \\end{equation}\n",
    "\n",
    "    Where,\n",
    "    + $x_{FS} = x_{\\rm max} - x_{\\rm min}$\n",
    "    \n",
    "    + $\\Delta x$ is the smallest reasonable quantity that can be measured. In the accelerometer, this was the unit after digitalisation, here it is a bit more complicated because we have a very high resolution but a lot of noise. Therefore here we take into account the noise and we consider $\\Delta x = std(x)$.\n",
    "    \n",
    "    \\begin{equation}\n",
    "        D = 20 \\log_{10}(\\frac{x_{FS}}{\\Delta x}) = 20 \\log_{10}(\\frac{13-3}{0.1494}) =  36.5129 {\\rm dB}\n",
    "    \\end{equation}\n",
    "\n",
    "- ***Update Frequency*** : \n",
    "\n",
    "    Thymio updates the distance sensor at a frequency of 10 $Hz$ and generates the `prox` event after every update.\n",
    "\n",
    "\n",
    "- ***Precision*** : \n",
    "    \n",
    "    Precision can be defined as,\n",
    "\n",
    "    \\begin{equation}\n",
    "    {\\rm precision} = \\frac{\\rm range}{\\sigma} \\text{\t\tor simply $\\frac{1}{\\sigma}$}\n",
    "    \\end{equation}\n",
    "\n",
    "    Where $\\sigma$ is the standard deviation (SD) of measurements at a fixed distance.\n",
    "\n",
    "    In order to calculate the SD, an obstacle was fixed at a certain distance (6 cm while the experiment was conducted) and sensor value is recorded. The same procedure is repeated ten times. Using the results SD is calculated.\n",
    "    \n",
    "   \n",
    "| Trial | Sensor Value | Compute Distance [cm] |\n",
    "|:--------------:|:---------------------:|:------------------------------:|\n",
    "|        1       |          3657         |           6.135741008          |\n",
    "|        2       |          3649         |           6.159307155          |\n",
    "|        3       |          3749         |           5.864730315          |\n",
    "|        4       |          3732         |           5.914808378          |\n",
    "|        5       |          3744         |           5.879459157          |\n",
    "|        6       |          3728         |           5.926591451          |\n",
    "|        7       |          3635         |           6.200547913          |\n",
    "|        8       |          3638         |           6.191710608          |\n",
    "|        9       |          3703         |           6.000235661          |\n",
    "|       10       |          3767         |           5.811706484          |\n",
    "|                | SD = 50.7297 |       SD =0.1494      |\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "{\\rm precision} = \\frac{1}{0.1494} = 6.6934 \\text{\t$cm^{-1}$}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "- ***Resolution***\n",
    "\n",
    "    Resolution is the smallest input change that can be detected. In this case resolution would be the change in distance for a unit increment of the sensor value. We compute the resolution as\n",
    " \t\t\n",
    "    \\begin{equation}\n",
    "    {\\rm resolution} = \\frac{\\Delta distance}{\\Delta sensor\\_value} = \\frac{10 - 2}{5007 - 2039} \\approx  0.003 ~ \\text{cm}\n",
    "    \\end{equation}\n",
    "        \n",
    "\n",
    "- ***Accuracy***\n",
    "    \\begin{equation}\n",
    " \t\t{\\rm accuracy} = 1 - \\frac{|m -v |}{v}\n",
    " \t\\end{equation}\n",
    "    \n",
    "    Where,\n",
    " \t+ $m$ : Measured value\n",
    " \t+ $v$ : True value\n",
    "    \n",
    "| Trial | Compute Distance [cm] | Accuracy [\\%] |\n",
    "|:--------------:|:------------------------------:|:----------------------:|\n",
    "|        1       |           6.135741008          |       0.977376499      |\n",
    "|        2       |           6.159307155          |       0.973448807      |\n",
    "|        3       |           5.864730315          |       0.977455052      |\n",
    "|        4       |           5.914808378          |       0.985801396      |\n",
    "|        5       |           5.879459157          |       0.979909859      |\n",
    "|        6       |           5.926591451          |       0.987765242      |\n",
    "|        7       |           6.200547913          |       0.966575348      |\n",
    "|        8       |           6.191710608          |       0.968048232      |\n",
    "|        9       |           6.000235661          |       0.999960723      |\n",
    "|       10       |           5.811706484          |       0.968617747      |\n",
    "|                |           SD = 0.14943         |       Avg =  0.9784    |\n",
    "\n",
    "\n",
    "</span>\n",
    "</blockquote>\n",
    "\n",
    "***Is there cross-talk between the sensors? How can you verify this?***\n",
    "\n",
    "<blockquote>\n",
    "<span style=\"color: #2980B9 ;\">\n",
    "    \n",
    "Yes, there is cross-talk between the proximity sensors. One way of verifying this is by measuring a specific distance by placing an obstacle only in front of one sensor (you can observe if a sensor is active or not by looking at the red LEDs next to them) and repeating the same exercise while multiple sensors are being used. Observing the difference between the two cases can show there exists a cross-talk.\n",
    "\n",
    "</span>\n",
    "</blockquote>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local Navigation on Thymio\n",
    "\n",
    "\n",
    "Local navigation allows modulating the trajectory to avoid unforeseen, local obstacles. It often pushes the controller to leave the optimal path to make an avoidance manoeuvre. Once the obstacle is passed, the controller can go back to the optimal path or find a new one. An important issue is to decide when the controller has to start avoiding, and, more difficult, when the obstacle can be considered as passed.\n",
    "\n",
    "We will work with the Thymio placed on a surface that has a gray level gradient. We can consider that the darker spot is the goal. This allows, using the ground sensors, to measure the orientation to the goal.\n",
    "\n",
    "\n",
    "For this exercise, you should make use of the ``gradient.pdf`` file. \n",
    "\n",
    "## Moving Toward the Goal\n",
    "\n",
    "Implement on Thymio a program that makes it moving toward the goal, as illustrated in the figure below that shows for various positions the path that allows to go to the goal. For this, use the two ground sensors and a simple reactive behaviour.\n",
    "\n",
    "<br>\n",
    "\n",
    "![Filename](images/gotogoal.png)\n",
    "\n",
    "<br>\n",
    "\n",
    "<blockquote>\n",
    "<span style=\"color: #2980B9 ;\">\n",
    "    \n",
    "Here a simple code allowing to move toward the path:\n",
    "\n",
    "\n",
    "    var diff #difference between right and left ground sensor\n",
    "\n",
    "    onevent prox\n",
    "      diff = prox.ground.delta[1] - prox.ground.delta[0]\n",
    "      motor.left.target = BASICSPEED - diff*GAIN\n",
    "      motor.right.target = BASICSPEED + diff*GAIN\n",
    "\n",
    "\n",
    "Where BASICSPEED = 100 and GAIN = 10.\n",
    "\n",
    "</span>\n",
    "</blockquote>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And in python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-28T12:45:45.053844Z",
     "start_time": "2022-10-28T12:45:44.603974Z"
    }
   },
   "outputs": [],
   "source": [
    "import tdmclient.notebook\n",
    "await tdmclient.notebook.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-28T12:46:40.754008Z",
     "start_time": "2022-10-28T12:46:40.290085Z"
    }
   },
   "outputs": [],
   "source": [
    "%%run_python\n",
    "v = [32, 0, 32, 0, 32, 0, 32, 0]\n",
    "leds_circle = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-28T12:45:56.231271Z",
     "start_time": "2022-10-28T12:45:55.824924Z"
    }
   },
   "outputs": [],
   "source": [
    "%%run_python\n",
    "\n",
    "BASICSPEED = 100\n",
    "GAIN       = 10\n",
    "\n",
    "@onevent \n",
    "def prox():\n",
    "    global prox_ground_delta, motor_left_target, motor_right_target, BASICSPEED, GAIN\n",
    "    diff = prox_ground_delta[1] - prox_ground_delta[0]\n",
    "    motor_left_target = BASICSPEED - diff*GAIN\n",
    "    motor_right_target = BASICSPEED + diff*GAIN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-28T12:44:32.350604Z",
     "start_time": "2022-10-28T12:44:32.271393Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local Obstacle Avoidance\n",
    "\n",
    "Put an obstacle on the path, for instance the box of Thymio, as illustrated in the figure below. The Figure illustrates the paths that allow to go to the goal avoiding the obstacle. Implement a state machine with two states, one moving toward the goal like in the previous behaviour, another making obstacle avoidance. Which condition do you choose to decide to change from going toward the goal to obstacle avoidance behaviour? Which condition do you choose to decide to change back to moving toward the goal?\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "![Filename](images/avoid.png)\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "<blockquote>\n",
    "<span style=\"color: #2980B9 ;\">\n",
    "    \n",
    "\n",
    "Here is a possible solution with two distinct navigation behaviors: attraction to the goal with differential wheel velocities which depend on the measured difference of gray level on the ground (**state=0**), or obstacle avoidance using proximity sensors 0 and 4 to accelerate the wheel on the side where an obstacle is detected (**state=1**).\n",
    "\n",
    "    var speed0 = 100       # nominal speed\n",
    "    var speedGain = 2      # gain used with ground gradient\n",
    "    var obstThrL = 10      # low obstacle threshold to switch state 1->0\n",
    "    var obstThrH = 20      # high obstacle threshold to switch state 0->1\n",
    "    var obstSpeedGain = 5  # /100 (actual gain: 5/100=0.05)\n",
    "\n",
    "    var state = 0          # 0=gradient, 1=obstacle avoidance\n",
    "    var diffDelta          # difference between ground sensors\n",
    "    var obst[2]            # measurements from left and right prox sensors\n",
    "\n",
    "    timer.period[0] = 10   # 10ms sampling time\n",
    "\n",
    "    onevent timer0\n",
    "      # acquisition from ground sensor for going toward the goal\n",
    "      diffDelta = prox.ground.delta[1] - prox.ground.delta[0]\n",
    "      # acquisition from the proximity sensors to detect obstacles\n",
    "      obst = [prox.horizontal[0], prox.horizontal[4]]\n",
    "      if state == 0 and (obst[0] > obstThrH or obst[1] > obstThrH) then\n",
    "        # switch from goal tracking to obst avoidance if obstacle detected\n",
    "        state = 1\n",
    "      elseif state == 1 and obst[0] < obstThrL and obst[1] < obstThrL then\n",
    "        # switch from obst avoidance to goal tracking if obstacle got unseen\n",
    "        state = 0\n",
    "      end\n",
    "      if  state == 0 then\n",
    "        # goal tracking: turn toward the goal\n",
    "        motor.left.target = speed0 - speedGain * diffDelta\n",
    "        motor.right.target = speed0 + speedGain * diffDelta\n",
    "      else\n",
    "        # obstacle avoidance: accelerate wheel near obstacle\n",
    "        motor.left.target = speed0 + obstSpeedGain * (obst[0] / 100)\n",
    "        motor.right.target = speed0 + obstSpeedGain * (obst[1] / 100)\n",
    "      end\n",
    "\n",
    "\n",
    "</span>\n",
    "</blockquote>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and using Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-01T19:58:24.378081Z",
     "start_time": "2021-10-01T19:57:58.602Z"
    }
   },
   "outputs": [],
   "source": [
    "%%run_python\n",
    "\n",
    "speed0 = 100       # nominal speed\n",
    "speedGain = 2      # gain used with ground gradient\n",
    "obstThrL = 10      # low obstacle threshold to switch state 1->0\n",
    "obstThrH = 20      # high obstacle threshold to switch state 0->1\n",
    "obstSpeedGain = 5  # /100 (actual gain: 5/100=0.05)\n",
    "\n",
    "state = 1          # 0=gradient, 1=obstacle avoidance\n",
    "obst = [0,0]       # measurements from left and right prox sensors\n",
    "\n",
    "timer_period[0] = 10   # 10ms sampling time\n",
    "\n",
    "\n",
    "@onevent \n",
    "def timer0():\n",
    "    global prox_ground_delta, prox_horizontal, motor_left_target, motor_right_target, state, obst, obstThrH, obstThrL, obstSpeedGain, speed0, speedGain \n",
    "    # acquisition from ground sensor for going toward the goal\n",
    "    diffDelta = prox_ground_delta[1] - prox_ground_delta[0]\n",
    "\n",
    "    # acquisition from the proximity sensors to detect obstacles\n",
    "    obst = [prox_horizontal[0], prox_horizontal[4]]\n",
    "    \n",
    "    # tdmclient does not support yet multiple and/or in if statements:\n",
    "    if state == 0: \n",
    "        # switch from goal tracking to obst avoidance if obstacle detected\n",
    "        if (obst[0] > obstThrH):\n",
    "            state = 1\n",
    "        elif (obst[1] > obstThrH):\n",
    "            state = 1\n",
    "    elif state == 1:\n",
    "        if obst[0] < obstThrL:\n",
    "            if obst[1] < obstThrL : \n",
    "                # switch from obst avoidance to goal tracking if obstacle got unseen\n",
    "                state = 0\n",
    "    if  state == 0 :\n",
    "        # goal tracking: turn toward the goal\n",
    "        leds_top = [0,0,0]\n",
    "        motor_left_target = speed0 - speedGain * diffDelta\n",
    "        motor_right_target = speed0 + speedGain * diffDelta\n",
    "    else:\n",
    "        leds_top = [30,30,30]\n",
    "        # obstacle avoidance: accelerate wheel near obstacle\n",
    "        motor_left_target = speed0 + obstSpeedGain * (obst[0] // 100)\n",
    "        motor_right_target = speed0 + obstSpeedGain * (obst[1] // 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Potential Field Navigation\n",
    "\n",
    "Implement the local obstacle avoidance behaviour using the potential field approach.\n",
    "\n",
    "<blockquote>\n",
    "<span style=\"color: #2980B9 ;\">\n",
    "    \n",
    "The measurements obtained from the ground sensors to get the direction of the goal and from the proximity sensors to avoid the obstacles can be combined into a single 2D vector, which depends on the robot position: this defines a 2D vector field. If the vector field is conservative (integrals between two points are independent of the path), it's the gradient of a potential field.\n",
    "\n",
    "More precisely, because differentiation is a linear operation, the contribution of each sensor measurement can be considered separately and the results added:\n",
    "\n",
    "+ The ground sensors measure directly a value proportional to the potential. If the two sensors give the same value, they are on the same level curve and the robot continues forward; if they give different value, potential differentiation (discrete difference) will make the robot turn left or right, toward the goal.\n",
    "\n",
    "\n",
    "+ Each proximity sensor measures a value larger if an obstacle is close. This gives a vector in the 2D plane in the direction of the sensor, with a contribution backward (mostly for the front sensors) and lateral (mostly for lateral sensors), whose total brings the robot away from the obstacle.\n",
    "\n",
    "\n",
    "The resultant force is used to modify the nominal speed of the robot (the desired speed for a flat potential).\n",
    "\n",
    "In the program below, we consider only the final result from the ground and proximity sensors.\n",
    "\n",
    "\n",
    "    var speed0 = 100      # nominal speed\n",
    "    var speedGain = 2     # gain used with ground gradient\n",
    "    var obstSpeedGain[] = [6, 4, -2, -6, -8]\t# /100\n",
    "                          # gains used with front proximity sensors 0..4\n",
    "\n",
    "    var diffDelta         # difference between ground sensors\n",
    "    var spLeft            # velocity of left wheel\n",
    "    var spRight           # velocity of right wheel\n",
    "    var i                 # for loop\n",
    "\n",
    "    timer.period[0] = 10  # 10ms sampling time\n",
    "\n",
    "    onevent timer0\n",
    "      # acquisition from ground sensor\n",
    "      diffDelta = prox.ground.delta[1] - prox.ground.delta[0]\n",
    "      # speed based on nominal velocity and ground (gradient due to goal)\n",
    "      spLeft = speed0 - speedGain * diffDelta\n",
    "      spRight = speed0 + speedGain * diffDelta\n",
    "      # adjustment for obstacles (\"gradient\" due to obstacles)\n",
    "      for i in 0:4 do\n",
    "        spLeft += prox.horizontal[i] * obstSpeedGain[i] / 100\n",
    "        spRight += prox.horizontal[i] * obstSpeedGain[4 - i] / 100\n",
    "      end\n",
    "      # motor control\n",
    "      motor.left.target = spLeft\n",
    "      motor.right.target = spRight\n",
    "\n",
    "</span>\n",
    "</blockquote>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-03T20:38:32.791058Z",
     "start_time": "2021-09-03T20:38:32.757678Z"
    }
   },
   "source": [
    "Using Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-01T19:58:24.379680Z",
     "start_time": "2021-10-01T19:57:58.605Z"
    }
   },
   "outputs": [],
   "source": [
    "%%run_python\n",
    "\n",
    "speed0 = 100      # nominal speed\n",
    "speedGain = 2     # gain used with ground gradient\n",
    "obstSpeedGain = [6, 4, -2, -6, -8]    # /100\n",
    "                  # gains used with front proximity sensors 0..4\n",
    "\n",
    "timer_period[0] = 10  # 10ms sampling time\n",
    "\n",
    "@onevent \n",
    "def timer0():\n",
    "    global prox_ground_delta, prox_horizontal, speed0, speedGain,obstSpeedGain, motor_left_target, motor_right_target\n",
    "    # acquisition from ground sensor\n",
    "    diffDelta = prox_ground_delta[1] - prox_ground_delta[0]\n",
    "    \n",
    "    # speed based on nominal velocity and ground (gradient due to goal)\n",
    "    spLeft = speed0 - speedGain * diffDelta\n",
    "    spRight = speed0 + speedGain * diffDelta\n",
    "    \n",
    "    # adjustment for obstacles (\"gradient\" due to obstacles)\n",
    "    for i in range(5):\n",
    "        spLeft += prox_horizontal[i] * obstSpeedGain[i] // 100\n",
    "        spRight += prox_horizontal[i] * obstSpeedGain[4 - i] // 100\n",
    "    # motor control\n",
    "    motor_left_target = spLeft\n",
    "    motor_right_target = spRight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Optional) Using Sensor Values and Displacements to Map the Environment\n",
    "\n",
    "The goal of this exercise is two-fold:\n",
    "\n",
    "\n",
    "- to implement the local occupancy grid around the Thymio robot based on the sensor readings. \n",
    "\n",
    "\n",
    "- provided a set of displacements and sensor readings, estimate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-01T19:58:02.596349Z",
     "start_time": "2021-10-01T19:58:01.118491Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install numpy matplotlib pandas scipy tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a local occupancy grid based on the individual sensor values\n",
    "\n",
    "Obstacle avoidance algorithms rely on knowing where the obstacles in the environment are. \n",
    "The goal of this exercise is to determine the position of obstacles detected by a virtual Thymio's sensors and place them in a local occupancy grid. \n",
    "\n",
    "You will have to start by estimating the distance of the different obstacles in the environment using the proximity sensor measurements that are provided. \n",
    "\n",
    "Therefore, you need to convert the sensor values into distances and place them in the grid around the robot. As such, you must take into account the geometry of the robot and the position / orientation of the different sensors. \n",
    "\n",
    "\n",
    "------------------------------------------------------------------\n",
    "------------------------------------------------------------------\n",
    "First we start by loading some of the standard python libraries and others that we will use in the exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-01T19:58:03.546672Z",
     "start_time": "2021-10-01T19:58:02.599165Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import math\n",
    "from statistics import mean\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "sys.path.insert(0, os.path.join(os.getcwd(), 'src'))\n",
    "\n",
    "from local_occupancy import sensor_measurements, sensor_distances\n",
    "from local_occupancy import thymio_coords, sensor_pos_from_center, sensor_angles\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this exercise, we are going to give you a list of : \n",
    "\n",
    "- fictional sensor values : `sensor_measurements`\n",
    "\n",
    "- corresponding distance measurements  : `sensor_distances`\n",
    "\n",
    "\n",
    "As we are going to be giving you quite a few variables in this exercise, we are giving you a function called **`variable_info`**. This function will print the type, content and elements that can be accessed from the variable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-01T19:58:03.554465Z",
     "start_time": "2021-10-01T19:58:03.548665Z"
    }
   },
   "outputs": [],
   "source": [
    "def variable_info(variable):\n",
    "    \"\"\"\n",
    "    Provided a variable, prints the type and content of the variable\n",
    "    \"\"\"\n",
    "    print(\"This variable is a {}\".format(type(variable)))\n",
    "    if type(variable) == np.ndarray:\n",
    "        print(\"\\n\\nThe shape is {}\".format(variable.shape))\n",
    "    print(\"\\n\\nThe data contained in the variable is : \")\n",
    "    print(variable)\n",
    "    print(\"\\n\\nThe elements that can be accessed in the variable are :\\n\")\n",
    "    print(dir(variable))\n",
    "    \n",
    "variable_info(np.array([1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start easy and plot the sensor_distances w.r.t the sensor measurements \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-01T19:58:03.783823Z",
     "start_time": "2021-10-01T19:58:03.557214Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,5))\n",
    "plt.plot(sensor_measurements, sensor_distances)\n",
    "plt.ylabel(\"Distance in cm\")\n",
    "plt.xlabel(\"Sensor values\")\n",
    "plt.title(\"Distance corresponding to the sensor value\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now complete the function below that will give you the distance of the different obstacles in the environment based on the proximity sensor measurements. Have a look at the `interp1d` function that scipy has to provide. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-01T19:58:03.791812Z",
     "start_time": "2021-10-01T19:58:03.785782Z"
    }
   },
   "outputs": [],
   "source": [
    "## Interpolation from sensor values to distances in cm\n",
    "def sensor_val_to_cm_dist(val):\n",
    "    \"\"\"\n",
    "    Returns the distance corresponding to the sensor value based \n",
    "    on the sensor characteristics\n",
    "    :param val: the sensor value that you want to convert to a distance\n",
    "    :return: corresponding distance in cm\n",
    "    \"\"\"\n",
    "    if val == 0:\n",
    "        return np.inf\n",
    "    \n",
    "    f = interp1d(sensor_measurements, sensor_distances)\n",
    "    return f(val).item()\n",
    "\n",
    "# Verifying the interpolation\n",
    "sensor_val_to_cm_dist(4996)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have verified that the function works, the goal is to create the local occupancy grid that takes into account the geometry of the robot. \n",
    "\n",
    "This means, we would like it if you plotted the position of the obstacles in the grid surrounding the robot. For this you need the following information :\n",
    "\n",
    "- The position of each sensor with respect to the center of the robot. This is provided in the variable `sensor_pos_from_center` : a list containing the coordinates of the 7 sensors proximity sensors starting from the top left to the bottom left in clockwise direction. (0,0) corresponds to the center of the robot\n",
    "\n",
    "- The orientation of each of the sensors, provided under `sensor_angles` : a list containing the angle that each sensor does with respect to the x axis. \n",
    "\n",
    "- The position of the comtour of the robot w.r.t its center, provided under `thymio_coords`, for the visualisation of the occupancy grid: a list of coordinates making up the outline of the Thymio robot. (0,0) corresponds to the center of the robot. \n",
    "\n",
    "\n",
    "We have provided the prototype of the function and of the plots below. Please update it to get the results shown in the image below with the sensor values provided in the code cell. \n",
    "\n",
    "<img src=\"Images/part1_solution.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-01T19:58:04.117590Z",
     "start_time": "2021-10-01T19:58:03.793682Z"
    }
   },
   "outputs": [],
   "source": [
    "def obstacles_pos_from_sensor_vals(sensor_vals):\n",
    "    \"\"\"\n",
    "    Returns a list containing the position of the obstacles\n",
    "    w.r.t the center of the Thymio robot. \n",
    "    :param sensor_vals: sensor values provided clockwise starting from the top left sensor.\n",
    "    :return: numpy.array() that contains the position of the different obstacles\n",
    "    \"\"\"\n",
    "    dist_to_sensor = [sensor_val_to_cm_dist(x) for x in sensor_vals]\n",
    "    dx_from_sensor = [d*math.cos(alpha) for (d, alpha) in zip(dist_to_sensor, sensor_angles)]\n",
    "    dy_from_sensor = [d*math.sin(alpha) for (d, alpha) in zip(dist_to_sensor, sensor_angles)]\n",
    "    obstacles_pos = [[x[0]+dx, x[1]+dy] for (x,dx,dy) in zip(sensor_pos_from_center,dx_from_sensor,dy_from_sensor )]\n",
    "    return np.array(obstacles_pos)\n",
    "\n",
    "sensor_vals = [1400, 3000, 5000, 500, 0, 5000, 1400]\n",
    "\n",
    "obstacles_pos = obstacles_pos_from_sensor_vals(sensor_vals)\n",
    "\n",
    "plt.figure(figsize=(7,7))\n",
    "plt.title(\"Local occupancy grid placing the detected obstacles arround the Thymio\")\n",
    "plt.xlabel(\"Distance in cm perpendicular to the Thymio orientation\")\n",
    "plt.ylabel(\"Distance in cm parallel to the Thymio orientation\")\n",
    "\n",
    "plt.plot(thymio_coords[:,0], thymio_coords[:,1])\n",
    "plt.axis(\"equal\")\n",
    "plt.scatter(obstacles_pos[:,0], obstacles_pos[:,1], marker=\"o\", color=\"r\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the global map provided the sensor values and robot displacements within the map.\n",
    "\n",
    "Now that you are able to construct the local occupancy grid, the goal is to remember where the obstacles are and create a global map. Here, we give you a set of relative displacements (`rel_dpos`) and corresponding sensor values (`map_sensor_vals`). \n",
    "\n",
    "The goal is that you use this information to construct the global map. \n",
    "\n",
    "\n",
    "\n",
    "Let's start by constructing a function that rotates a set of coordinates by the given angle. You can test it out on the drawing of the thymio's coordinates to make sure it works. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-01T19:58:04.124381Z",
     "start_time": "2021-10-01T19:58:04.120699Z"
    }
   },
   "outputs": [],
   "source": [
    "def rotate(angle, coords):\n",
    "    \"\"\"\n",
    "    Rotates the coordinates of a matrix by the desired angle\n",
    "    :param angle: angle in radians by which we want to rotate\n",
    "    :return: numpy.array() that contains rotated coordinates\n",
    "    \"\"\"\n",
    "    R = np.array(((np.cos(angle), -np.sin(angle)),\n",
    "                  (np.sin(angle),  np.cos(angle))))\n",
    "    \n",
    "    return R.dot(coords.transpose()).transpose()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have done that, we can move on to constructing a global map from the relative displacements and corresponding sensor values. \n",
    "\n",
    "------\n",
    "------\n",
    "One way of solving this problem is to do the following (but it is not the only way so if you prefer adopting another solution or changing any portion of the pseudo-code feel free to do so) :\n",
    "\n",
    "1. Start by defining the initial position of the robot as the origin of the map (i.e the position x,y,theta = 0, 0, 0)\n",
    "\n",
    "\n",
    "2. Provided the relative displacements (rel_dpos), compute the absolute position of the robot at each step\n",
    "\n",
    "\n",
    "3. Compute the local occupancy grid from the sensor values at each step. Use the function that you implemented previously\n",
    "\n",
    "\n",
    "4. Create the global map from the local occupancy grids by rotating and translating the obstacles found. To do so, for each data point you will have to :\n",
    "    - Rotate the local occupancy grid and coordinates of the outline of the thymio\n",
    "    - Translate the local occupancy grid and the coordinates of the outline to the position of the Thymio\n",
    "    - Store the coordinates in a list\n",
    "\n",
    "5. Plot on a final figure :\n",
    "    - the trajectory taken by the robot\n",
    "    - the outline of the robot at each step\n",
    "    - the position of the obstacles that were seen by the robot in the global frame\n",
    "\n",
    "-----\n",
    "-----\n",
    "\n",
    "Here is the result you should get : \n",
    "\n",
    "<img src=\"Images/map_creation_solution.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-01T19:58:04.621267Z",
     "start_time": "2021-10-01T19:58:04.126390Z"
    }
   },
   "outputs": [],
   "source": [
    "from local_occupancy import map_sensor_vals, rel_dpos\n",
    "\n",
    "###------------------------------------------------------\n",
    "# STEP 1\n",
    "###------------------------------------------------------\n",
    "\n",
    "# Arbitrarily define the initial robot position as the origin of the map\n",
    "abs_pos = [[0,0, math.pi/2]] # List of lists that will contain the absolute x,y,theta coordinates of the robot\n",
    "\n",
    "###------------------------------------------------------\n",
    "# STEP 2\n",
    "###------------------------------------------------------\n",
    "\n",
    "# Provided the relative positions, compute the absolute positions at each step\n",
    "for (dx,dy,dtheta) in rel_dpos[:]:\n",
    "    (x,y,theta) = abs_pos[-1][0], abs_pos[-1][1], abs_pos[-1][2]\n",
    "    d = np.sqrt(dx**2+dy**2)\n",
    "    new_pos = [x+d*np.cos(theta+dtheta), y+d*np.sin(theta+dtheta), (theta+dtheta)%(2*math.pi)]\n",
    "    # Appending the computed absolute x, y and theta coordinates to the list\n",
    "    abs_pos.append(new_pos)\n",
    "\n",
    "abs_pos = np.array(abs_pos)\n",
    "\n",
    "###------------------------------------------------------\n",
    "# STEP 3\n",
    "###------------------------------------------------------\n",
    "\n",
    "# Compute the local occupancy grid from the sensor values at each step\n",
    "local_occupancy_grids = [obstacles_pos_from_sensor_vals(x) for x in map_sensor_vals]\n",
    "\n",
    "###------------------------------------------------------\n",
    "# STEP 4\n",
    "###------------------------------------------------------\n",
    "\n",
    "# Create the global map based on the data acquired previously\n",
    "global_map, overall_thymio_coords = [], []\n",
    "\n",
    "for (local_grid, pos) in zip(local_occupancy_grids, abs_pos):\n",
    "    \n",
    "    # Rotate the local occupancy grid\n",
    "    rotated_grid = rotate(pos[2]-math.pi/2, local_grid)\n",
    "    rotated_thymio_coords = rotate(pos[2]-math.pi/2, thymio_coords)\n",
    "    \n",
    "    # Translate the grid at the position of the Thymio\n",
    "    obstacles_pos = rotated_grid+np.array([pos[0], pos[1]])\n",
    "    abs_Thymio_coords = rotated_thymio_coords+np.array([pos[0], pos[1]])\n",
    "    \n",
    "    # Store position of the obstacles and Thymio in the global map\n",
    "    global_map.append(obstacles_pos)\n",
    "    overall_thymio_coords.append(abs_Thymio_coords)\n",
    "\n",
    "###------------------------------------------------------\n",
    "# STEP 5\n",
    "###------------------------------------------------------\n",
    "\n",
    "global_map = np.array(np.vstack(global_map))\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.plot(abs_pos[:,0], abs_pos[:,1])\n",
    "plt.scatter(global_map[:,0],global_map[:,1], color=\"r\", s=10)\n",
    "\n",
    "plt.plot(np.array(abs_pos)[:,0], \n",
    "        np.array(abs_pos)[:,1], color=\"r\", marker=\"o\")\n",
    "\n",
    "for coords in overall_thymio_coords:\n",
    "    plt.plot(coords[:,0], coords[:,1], color=\"g\")\n",
    "    \n",
    "plt.axis(\"equal\");"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
